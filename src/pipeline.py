"""
Main file to execute the TRS Pipeline.

The test queries can be found under "../tests/prompts.json. These were generated by ChatGPT using the following prompt: 

I want to test a travel recommendation system. The system requests prompts from the user about what their travel interests are, and when they plan to travel (this can either be a particular season or month(s) and is optional). The system then recommends cities in Europe based on these user prompts. Your job is to generate 10 different prompts in order to test this recommendation system. Here's an example of what a prompt looks like: 

"Suggest some places to visit during winter. I like hiking, nature and the mountains and I enjoy skiing in winter."

What are your 10 test prompts? Generate them in JSON.

"""

from augmentation import prompt_generation as pg
from information_retrieval import info_retrieval as ir
from text_generation.model_init import (
    Llama3,
    Mistral,
    Gemma2,
    Llama3Point1,
)
from text_generation import text_generation as tg
import logging 

logger = logging.getLogger(__name__)
logging.basicConfig(encoding='utf-8', level=logging.DEBUG)

TEST_DIR = "../tests/"
MODELS = {
        'Llama3': Llama3, 
        'Mistral': Mistral, 
        'Gemma2': Gemma2, 
        'Llama3.1': Llama3Point1,
    }

def pipeline(query, model_name, **params):
    """
    
    Executes the entire RAG pipeline, provided the query and model class name.

    Args: 
        - query: str
        - model_name: string, one of the following: Llama3, Mistral, Gemma2, Llama3Point1
        - params: 
            - limit (number of results to be retained) 
            - reranking (binary, whether to rerank results using ColBERT or not)
            - sustainability

    
    """

    model = MODELS[model_name]

    context_params = {
        'limit': 5,
        'reranking': 0,
        'sustainability': 0
    }

    if 'limit' in params:
        context_params['limit'] = params['limit'] 
    
    if 'reranking' in params: 
        context_params['reranking'] = params['reranking']
    
    if 'sustainability' in params: 
        context_params['sustainability'] = params['sustainability']


    logger.info("Retrieving context..")
    try:
        context = ir.get_context(query=query, **context_params)
    except Exception as e:
        logger.error(f"Error while trying to get context: {e}")
        return None
    
    logger.info("Retrieved context, augmenting prompt..")
    try:
        prompt = pg.augment_prompt(
            query=query, 
            context=context,
            sustainability=0,
            params=context_params
        )
    except Exception as e:
        logger.error(f"Error while trying to augment prompt: {e}")
        return None
    
    # return without_sfairness

    logger.info(f"Augmented prompt, initializing {model} and generating response..")
    try:
        response = tg.generate_response(model, prompt)
    except Exception as e: 
        logger.info(f"Error while generating response: {e}")
        return None

    return response